{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/co-dutt1/.conda/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "def enable_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "def check_tunable_params(model, verbose=True):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            if(verbose):\n",
    "                print(name)\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.5f}\"\n",
    "    )\n",
    "\n",
    "    return trainable_params, all_param\n",
    "\n",
    "def create_mapping(model, vector):\n",
    "    mapping = {}\n",
    "    i = 0\n",
    "\n",
    "    for name_p,p in model.named_parameters():\n",
    "        if '.attn.' in name_p or 'attention' in name_p:\n",
    "            mapping[name_p] = vector[i]\n",
    "            i += 1\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    return mapping\n",
    "\n",
    "def sort_dict(dict, descending=False):\n",
    "    sorted_dict = dict(sorted(dict.items(), key=lambda item: item[1], reverse=descending))\n",
    "    \n",
    "    return sorted_dict\n",
    "\n",
    "def get_modules_from_vector(vector, model):\n",
    "    trainable_blocks = []\n",
    "    frozen_blocks = []\n",
    "    \n",
    "    trainable_blocks = np.where(np.array(vector) == 1)\n",
    "    frozen_blocks = np.where(np.array(vector) == 0)\n",
    "    \n",
    "    return trainable_blocks, frozen_blocks\n",
    "\n",
    "def get_model_for_bitfit(model):\n",
    "    trainable_components = ['bias', 'pooler.dense.bias', 'head'] \n",
    "\n",
    "    # Disale all the gradients\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False \n",
    "      \n",
    "    vector = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        for component in trainable_components:\n",
    "            if component in name:\n",
    "                vector.append(1)\n",
    "                param.requires_grad = True\n",
    "                break\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def enable_from_vector(vector, model):\n",
    "    print(\"Vector: \", vector)\n",
    "    \n",
    "    disable_module(model)\n",
    "    \n",
    "    for idx, block in enumerate(model.blocks): \n",
    "    \n",
    "        if(vector[idx] == 1):\n",
    "            print(\"Enabling attention in Block {}\".format(idx))\n",
    "            enable_module(block.attn)\n",
    "        else:\n",
    "            #print(\"Disabling attention in Block {}\".format(idx))\n",
    "            disable_module(block.attn)\n",
    "\n",
    "def create_best_worst_vectors(df, k=10):\n",
    "    \n",
    "    best_df = df.sort_values(by=['Test Acc@1'], ascending=False).head(k).reset_index(drop=True)\n",
    "    worst_df = df.sort_values(by=['Test Acc@1'], ascending=True).head(k).reset_index(drop=True)\n",
    "\n",
    "    best_vector = np.array([0]*12)\n",
    "\n",
    "    for i in range(len(best_df)):\n",
    "        vector_path = best_df['Vector Path'][i]\n",
    "        vector = np.load(vector_path)\n",
    "        best_vector += vector\n",
    "\n",
    "    worst_vector = np.array([0]*12)\n",
    "\n",
    "    for i in range(len(worst_df)):\n",
    "        vector_path = worst_df['Vector Path'][i]\n",
    "        vector = np.load(vector_path)\n",
    "        worst_vector += vector\n",
    "\n",
    "    return best_vector, worst_vector\n",
    "\n",
    "def tune_blocks_random(model, mask, segment):\n",
    "\n",
    "    vector = []\n",
    "\n",
    "    for idx, block in enumerate(model.blocks):\n",
    "\n",
    "        if(mask is None):\n",
    "            bit = int(np.random.random(1)[0] > 0.5)\n",
    "        else:\n",
    "            bit = mask[idx]\n",
    "\n",
    "        if(bit == 1):\n",
    "            print(\"Enabling {} in Block {}\".format(segment, idx))\n",
    "            if(segment == 'attention'):\n",
    "                enable_module(block.attn)\n",
    "            elif(segment == 'layernorm'):\n",
    "                enable_module(block.norm1)\n",
    "                enable_module(block.norm2)\n",
    "\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            print(\"Disabling {} in Block {}\".format(segment, idx))\n",
    "            if(segment == 'attention'):\n",
    "                disable_module(block.attn)\n",
    "            elif(segment == 'layernorm'):\n",
    "                disable_module(block.norm1)\n",
    "                disable_module(block.norm2)\n",
    "            \n",
    "            vector.append(0)\n",
    "    \n",
    "    if(mask is not None):\n",
    "        assert (mask == vector)\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vision Transformer (ViT) in PyTorch\n",
    "\n",
    "A PyTorch implement of Vision Transformers as described in:\n",
    "\n",
    "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n",
    "    - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n",
    "    - https://arxiv.org/abs/2106.10270\n",
    "\n",
    "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
    "\n",
    "Acknowledgments:\n",
    "* The paper authors for releasing code and weights, thanks!\n",
    "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
    "for some einops/einsum fun\n",
    "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
    "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
    "\n",
    "Hacked together by / Copyright 2020, Ross Wightman\n",
    "\"\"\"\n",
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.models.helpers import build_model_with_cfg, named_apply, adapt_input_conv, resolve_pretrained_cfg, checkpoint_seq\n",
    "from timm.models.layers import DropPath, trunc_normal_, lecun_normal_, _assert\n",
    "from timm.models.layers.helpers import to_2tuple\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "\n",
    "\n",
    "import ipdb\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n",
    "        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models (weights from official Google JAX impl)\n",
    "    \n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://storage.googleapis.com/vit_models/augreg/'\n",
    "            'B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n",
    "}\n",
    "\n",
    "'''\n",
    "New Linear layers that allows updating weights manually\n",
    "'''\n",
    "class Linear_fw(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=None):\n",
    "        super(Linear_fw, self).__init__(in_features, out_features)\n",
    "        self.weight.fast = None  # Lazy hack to add fast weight link\n",
    "        self.bias.fast = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.weight.fast is not None and self.bias.fast is not None:\n",
    "            # weight.fast (fast weight) is the temporarily adapted weight\n",
    "            out = F.linear(x, self.weight.fast, self.bias.fast)\n",
    "        else:\n",
    "            out = super(Linear_fw, self).forward(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        bias = to_2tuple(bias)\n",
    "        drop_probs = to_2tuple(drop)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(drop_probs[0])\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1])\n",
    "        self.drop2 = nn.Dropout(drop_probs[1])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x) \n",
    "        \n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., meta=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.meta = meta\n",
    "\n",
    "        if(self.meta):\n",
    "            self.qkv = Linear_fw(dim, dim * 3, bias=qkv_bias)\n",
    "            self.proj = Linear_fw(dim, dim)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "            self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None,\n",
    "            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, meta=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, meta=meta)\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResPostBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., init_values=None,\n",
    "            drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.init_values = init_values\n",
    "\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # NOTE this init overrides that base model init with specific changes for the block type\n",
    "        if self.init_values is not None:\n",
    "            nn.init.constant_(self.norm1.weight, self.init_values)\n",
    "            nn.init.constant_(self.norm2.weight, self.init_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.norm1(self.attn(x)))\n",
    "        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, dim, num_heads, num_parallel=2, mlp_ratio=4., qkv_bias=False, init_values=None,\n",
    "            drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.num_parallel = num_parallel\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for _ in range(num_parallel):\n",
    "            self.attns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('attn', Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "            self.ffns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('mlp', Mlp(dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "\n",
    "    def _forward_jit(self, x):\n",
    "        x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)\n",
    "        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)\n",
    "        return x\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _forward(self, x):\n",
    "        x = x + sum(attn(x) for attn in self.attns)\n",
    "        x = x + sum(ffn(x) for ffn in self.ffns)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.jit.is_scripting() or torch.jit.is_tracing():\n",
    "            return self._forward_jit(x)\n",
    "        else:\n",
    "            return self._forward(x)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" 2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.flatten = flatten\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        _assert(H == self.img_size[0], f\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\")\n",
    "        _assert(W == self.img_size[1], f\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\")\n",
    "\n",
    "        x = self.proj(x) \n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n",
    "        - https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, global_pool='token',\n",
    "            embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, init_values=None,\n",
    "            class_token=True, fc_norm=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., weight_init='',\n",
    "            embed_layer=PatchEmbed, norm_layer=None, act_layer=None, block_fn=Block, meta=False): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            global_pool (str): type of global pooling for final sequence (default: 'token')\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            init_values: (float): layer-scale init values\n",
    "            class_token (bool): use class token\n",
    "            fc_norm (Optional[bool]): pre-fc norm after pool, set if global_pool == 'avg' if None (default: None)\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            weight_init (str): weight init scheme\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "            act_layer: (nn.Module): MLP activation layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert global_pool in ('', 'avg', 'token')\n",
    "        assert class_token or global_pool != 'token'\n",
    "        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 1 if class_token else 0\n",
    "        self.grad_checkpointing = False \n",
    "\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if self.num_tokens > 0 else None\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + self.num_tokens, embed_dim) * .02)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            block_fn(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, init_values=init_values,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer, meta=meta)\n",
    "            for i in range(depth)])\n",
    "\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
    "\n",
    "        # Classifier Head\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if weight_init != 'skip':\n",
    "            self.init_weights(weight_init)\n",
    "\n",
    "    def init_weights(self, mode=''):\n",
    "        assert mode in ('jax', 'jax_nlhb', 'moco', '')\n",
    "        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        if self.cls_token is not None:\n",
    "            nn.init.normal_(self.cls_token, std=1e-6)\n",
    "        named_apply(get_init_weights_vit(mode, head_bias), self)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # this fn left here for compat with downstream users\n",
    "        init_weights_vit_timm(m)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'dist_token'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def group_matcher(self, coarse=False):\n",
    "        return dict(\n",
    "            stem=r'^cls_token|pos_embed|patch_embed',  # stem and embed\n",
    "            blocks=[(r'^blocks\\.(\\d+)', None), (r'^norm', (99999,))]\n",
    "        )\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes: int, global_pool=None):\n",
    "        self.num_classes = num_classes\n",
    "        if global_pool is not None:\n",
    "            assert global_pool in ('', 'avg', 'token')\n",
    "            self.global_pool = global_pool\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.cls_token is not None:\n",
    "            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "            \n",
    "        return x \n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        if self.global_pool:\n",
    "            x = x[:, self.num_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]\n",
    "        x = self.fc_norm(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "\n",
    "def init_weights_vit_timm(module: nn.Module, name: str = ''):\n",
    "    \"\"\" ViT weight initialization, original timm impl (for reproducibility) \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        trunc_normal_(module.weight, std=.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()\n",
    "\n",
    "\n",
    "def init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.):\n",
    "    \"\"\" ViT weight initialization, matching JAX (Flax) impl \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if name.startswith('head'):\n",
    "            nn.init.zeros_(module.weight)\n",
    "            nn.init.constant_(module.bias, head_bias)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()\n",
    "\n",
    "\n",
    "def init_weights_vit_moco(module: nn.Module, name: str = ''):\n",
    "    \"\"\" ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed \"\"\"\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if 'qkv' in name:\n",
    "            # treat the weights of Q, K, V separately\n",
    "            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))\n",
    "            nn.init.uniform_(module.weight, -val, val)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()\n",
    "\n",
    "\n",
    "def get_init_weights_vit(mode='jax', head_bias: float = 0.):\n",
    "    if 'jax' in mode:\n",
    "        return partial(init_weights_vit_jax, head_bias=head_bias)\n",
    "    elif 'moco' in mode:\n",
    "        return init_weights_vit_moco\n",
    "    else:\n",
    "        return init_weights_vit_timm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "    # NOTE representation layer has been removed, not used in latest 21k/1k pretrained weights\n",
    "    # if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "    #     model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "    #     model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if num_tokens:\n",
    "        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n",
    "        ntok_new -= num_tokens\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    if not len(gs_new):  # backwards compatibility\n",
    "        gs_new = [int(math.sqrt(ntok_new))] * 2\n",
    "    assert len(gs_new) >= 2\n",
    "    _logger.info('Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "        elif 'pre_logits' in k:\n",
    "            # NOTE representation layer removed as not used in latest 21k/1k pretrained weights\n",
    "            continue\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_vision_transformer(variant, pretrained=False, **kwargs):\n",
    "    if kwargs.get('features_only', None):\n",
    "        raise RuntimeError('features_only not implemented for Vision Transformer models.')\n",
    "\n",
    "    pretrained_cfg = resolve_pretrained_cfg(variant, pretrained_cfg=kwargs.pop('pretrained_cfg', None))\n",
    "    model = build_model_with_cfg(\n",
    "        VisionTransformer, variant, pretrained,\n",
    "        pretrained_cfg=pretrained_cfg,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        pretrained_custom_load='npz' in pretrained_cfg['url'],\n",
    "        **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch16_224(pretrained=False, **kwargs):\n",
    "    \"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (qkv): Linear_fw(in_features=768, out_features=2304, bias=True)\n",
       "  (proj): Linear_fw(in_features=768, out_features=768, bias=True)\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, meta=True)\n",
    "num_blocks = len(model.blocks)\n",
    "model.blocks[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention params in each block\n",
    "attn_params = []\n",
    "\n",
    "for i in range(num_blocks):\n",
    "    block_attn_params = []\n",
    "    block = model.blocks[i]\n",
    "    print(block.attn.qkv.weight.shape)\n",
    "    print(block.attn.qkv.bias.shape)\n",
    "    print(block.attn.proj.weight.shape)\n",
    "    print(block.attn.proj.bias.shape)\n",
    "    block_attn_params.append(block.attn.qkv.weight.detach().numpy())\n",
    "    block_attn_params.append(block.attn.qkv.bias.detach().numpy())\n",
    "    block_attn_params.append(block.attn.proj.weight.detach().numpy())\n",
    "    block_attn_params.append(block.attn.proj.bias.detach().numpy())\n",
    "\n",
    "    attn_params.append(block_attn_params)\n",
    "\n",
    "len(attn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_torch = torch.from_numpy(mask)\n",
    "\n",
    "#Performing elementwise multiplication\n",
    "result = []\n",
    "\n",
    "for i in range(12):\n",
    "    attn_sub_params_torch = [torch.from_numpy(elem) for elem in attn_params[i]]\n",
    "    result.append([elem * mask_torch[i] for elem in attn_sub_params_torch])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True, True, True, True],\n",
       " [True, True, True, True],\n",
       " [True, True, True, True],\n",
       " [True, True, True, True],\n",
       " [True, True, True, True],\n",
       " [True, True, True, True],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_zeros = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    all_zeros.append([torch.allclose(elem, torch.zeros_like(elem)) for elem in result[i]])\n",
    "\n",
    "all_zeros\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have performed elementwise multiplication successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0252, -0.0065, -0.0339,  ...,  0.0036,  0.0291, -0.0134],\n",
       "        [ 0.0335,  0.0025,  0.0293,  ...,  0.0111,  0.0007, -0.0090],\n",
       "        [-0.0370,  0.0088, -0.0099,  ...,  0.0021, -0.0323, -0.0390],\n",
       "        ...,\n",
       "        [-0.0132,  0.0502, -0.0103,  ..., -0.0079, -0.0195, -0.0269],\n",
       "        [ 0.0138,  0.0262, -0.0126,  ..., -0.0380, -0.0157,  0.0322],\n",
       "        [-0.0115, -0.0319,  0.0150,  ...,  0.0112, -0.0178,  0.0232]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].attn.qkv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_length = num_blocks\n",
    "mask = nn.Parameter(torch.randint(low=0, high=2, size=(mask_length,), dtype=torch.float32), \n",
    "                    requires_grad=True)\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    if('attn' in n):\n",
    "        block_index = n.split('.')[1]\n",
    "        p.fast = p - lr * mask[block_index] * p.grad          # Manual update of the fast weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, block in model.blocks:\n",
    "    for n, p in block.named_parameters():\n",
    "        if('attn' in n):\n",
    "            p.fast = p - lr*mask[k]*p.grad          # Manual update of the fast weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
