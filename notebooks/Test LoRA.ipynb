{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n",
    "        \n",
    "def enable_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "def check_tunable_params(model, verbose=True):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            if(verbose):\n",
    "                print(name)\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.5f}\"\n",
    "    )\n",
    "\n",
    "    return trainable_params, all_param\n",
    "\n",
    "def create_mapping(model, vector):\n",
    "    mapping = {}\n",
    "    i = 0\n",
    "\n",
    "    for name_p,p in model.named_parameters():\n",
    "        if '.attn.' in name_p or 'attention' in name_p:\n",
    "            mapping[name_p] = vector[i]\n",
    "            i += 1\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "            \n",
    "    return mapping\n",
    "\n",
    "def sort_dict(dict, descending=False):\n",
    "    sorted_dict = dict(sorted(dict.items(), key=lambda item: item[1], reverse=descending))\n",
    "    \n",
    "    return sorted_dict\n",
    "\n",
    "def get_modules_from_vector(vector, model):\n",
    "    trainable_blocks = []\n",
    "    frozen_blocks = []\n",
    "    \n",
    "    trainable_blocks = np.where(np.array(vector) == 1)\n",
    "    frozen_blocks = np.where(np.array(vector) == 0)\n",
    "    \n",
    "    return trainable_blocks, frozen_blocks\n",
    "\n",
    "def get_model_for_bitfit(model):\n",
    "    trainable_components = ['bias', 'pooler.dense.bias', 'head'] \n",
    "\n",
    "    # Disale all the gradients\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False \n",
    "      \n",
    "    vector = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        for component in trainable_components:\n",
    "            if component in name:\n",
    "                vector.append(1)\n",
    "                param.requires_grad = True\n",
    "                break\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def enable_from_vector(vector, model):\n",
    "    print(\"Vector: \", vector)\n",
    "    \n",
    "    disable_module(model)\n",
    "    \n",
    "    for idx, block in enumerate(model.blocks): \n",
    "    \n",
    "        if(vector[idx] == 1):\n",
    "            print(\"Enabling attention in Block {}\".format(idx))\n",
    "            enable_module(block.attn)\n",
    "        else:\n",
    "            #print(\"Disabling attention in Block {}\".format(idx))\n",
    "            disable_module(block.attn)\n",
    "\n",
    "def create_best_worst_vectors(df, k=10):\n",
    "    best_df = df.sort_values(by=['Test Acc@1'], ascending=False).head(k).reset_index(drop=True)\n",
    "    worst_df = df.sort_values(by=['Test Acc@1'], ascending=True).head(k).reset_index(drop=True)\n",
    "\n",
    "    best_vector = np.array([0]*12)\n",
    "\n",
    "    for i in range(len(best_df)):\n",
    "        vector_path = best_df['Vector Path'][i]\n",
    "        vector = np.load(vector_path)\n",
    "        best_vector += vector\n",
    "\n",
    "    worst_vector = np.array([0]*12)\n",
    "\n",
    "    for i in range(len(worst_df)):\n",
    "        vector_path = worst_df['Vector Path'][i]\n",
    "        vector = np.load(vector_path)\n",
    "        worst_vector += vector\n",
    "\n",
    "    return best_vector, worst_vector\n",
    "\n",
    "def tune_blocks_random(model, mask, segment):\n",
    "\n",
    "    vector = []\n",
    "\n",
    "    for idx, block in enumerate(model.blocks):\n",
    "\n",
    "        if(mask is None):\n",
    "            bit = int(np.random.random(1)[0] > 0.5)\n",
    "        else:\n",
    "            bit = mask[idx]\n",
    "\n",
    "        if(bit == 1):\n",
    "            print(\"Enabling {} in Block {}\".format(segment, idx))\n",
    "            if(segment == 'attention'):\n",
    "                enable_module(block.attn)\n",
    "            elif(segment == 'layernorm'):\n",
    "                enable_module(block.norm1)\n",
    "                enable_module(block.norm2)\n",
    "\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            print(\"Disabling {} in Block {}\".format(segment, idx))\n",
    "            if(segment == 'attention'):\n",
    "                disable_module(block.attn)\n",
    "            elif(segment == 'layernorm'):\n",
    "                disable_module(block.norm1)\n",
    "                disable_module(block.norm2)\n",
    "            \n",
    "            vector.append(0)\n",
    "    \n",
    "    if(mask is not None):\n",
    "        assert (mask == vector)\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(model, lora_r: int = 8, lora_alpha: int = 8, lora_dropout: float = 0., tune_k=False, block_mask=None):\n",
    "\n",
    "    lora_model = copy.deepcopy(model)\n",
    "    \n",
    "    tune_list = [True, True, True] if tune_k else [True, False, True]\n",
    "\n",
    "    block_mask = [1]*len(model.blocks) if block_mask is None else block_mask    #Apply LoRA to all attention layers if mask is not given.\n",
    "\n",
    "    for idx, block in enumerate(lora_model.blocks):\n",
    "        if(block_mask[idx] == 1):\n",
    "            in_d = block.attn.qkv.in_features\n",
    "            out_d = block.attn.qkv.out_features\n",
    "            block.attn.qkv = lora.MergedLinear(in_d, out_d, r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, enable_lora=tune_list)\n",
    "\n",
    "    lora_model.load_state_dict(model.state_dict(),strict=False)\n",
    "    lora.mark_only_lora_as_trainable(lora_model)\n",
    "    \n",
    "    \n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Mask:  [1 1 1 1 0 0 1 0 0 1 0 1]\n",
      "blocks.0.attn.qkv.lora_A\n",
      "blocks.0.attn.qkv.lora_B\n",
      "blocks.1.attn.qkv.lora_A\n",
      "blocks.1.attn.qkv.lora_B\n",
      "blocks.2.attn.qkv.lora_A\n",
      "blocks.2.attn.qkv.lora_B\n",
      "blocks.3.attn.qkv.lora_A\n",
      "blocks.3.attn.qkv.lora_B\n",
      "blocks.6.attn.qkv.lora_A\n",
      "blocks.6.attn.qkv.lora_B\n",
      "blocks.9.attn.qkv.lora_A\n",
      "blocks.9.attn.qkv.lora_B\n",
      "blocks.11.attn.qkv.lora_A\n",
      "blocks.11.attn.qkv.lora_B\n",
      "trainable params: 172032 || all params: 86739688 || trainable%: 0.19833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(172032, 86739688)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_mask = np.load('/home/co-dutt1/rds/hpc-work/Layer-Masking/Experiment_Vectors/random_vector_1.npy')\n",
    "print(\"Block Mask: \", block_mask)\n",
    "lora_model = create_lora_model(model, block_mask=block_mask)\n",
    "check_tunable_params(lora_model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
